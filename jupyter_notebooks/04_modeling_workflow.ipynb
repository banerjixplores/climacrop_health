{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f418c4d1",
   "metadata": {},
   "source": [
    "<h1 style=\"background-color:#4E9A06; color:#ffffff; padding:10px 15px; border-radius:5px; margin-top:1rem; margin-bottom:1rem; text-align:center;\">\n",
    "  04 – Modeling Workflow\n",
    "</h1>\n",
    "\n",
    "<b>Modeling Workflow Summary</b>\n",
    "This document outlines the machine learning workflow from the provided notebook. The process involves loading a cleaned dataset, engineering new features, building separate regression models for \"Agricultural\" and \"Wild\" ecosystems, evaluating their performance, and identifying the key drivers of disease incidence in each system.\n",
    "\n",
    "<h2 style=\"color: #4E9A06; margin-top: 1rem; margin-bottom: 0.5rem;\">\n",
    "Objectives\n",
    "</h2>\n",
    "<ul>\n",
    "  <li><strong>Understand</strong> the end-to-end modeling pipeline for predicting climate-driven disease incidence.</li>\n",
    "  <li><strong>Demonstrate</strong> feature engineering workflows, including raw and anomaly-based climate predictors and interaction terms.</li>\n",
    "  <li><strong>Compare</strong> multiple regression algorithms to assess predictive performance across wild and agricultural systems.</li>\n",
    "  <li><strong>Interpret</strong> model insights through cross-validation metrics and permutation-based feature importance.</li>\n",
    "</ul>\n",
    "\n",
    "<h2 style=\"color: #4E9A06; margin-top: 1rem; margin-bottom: 0.5rem;\">\n",
    "Inputs\n",
    "</h2>\n",
    "<ul>\n",
    "  <li><strong>Processed dataset:</strong> <code>data/processed/merged_climate_disease_final.csv</code> Site-year level DataFrame with raw incidence counts, climate measurements, and site metadata.</li>\n",
    "  <li><strong>Hyperparameter configurations: </strong>: Predefined grids for each regression algorithm.</li> \n",
    "  <li><strong>Modeling functions:  </strong> Pipeline and transformer definitions for feature preprocessing.</li>\n",
    "</ul>\n",
    "\n",
    "<h2 style=\"color: #4E9A06; margin-top: 1rem; margin-bottom: 0.5rem;\">\n",
    "Outputs\n",
    "</h2>\n",
    "<ul>\n",
    "  <li><strong>Prepared data matrices:</strong> Two optimized, final regression models (one for agricultural and one for wild systems), fitted on the respective training data.</li>\n",
    "  <li><strong>Trained models:</strong> Fitted pipelines for Ridge-spline, RandomForest, XGBoost, SVR, and a stacking ensemble.</li>\n",
    "  <li><strong>Evaluation metrics:</strong> Cross-validated R² scores and held-out test R² for both wild and agricultural subsets.</li>\n",
    "  <li><strong>Feature importance results:</strong> Permutation importance scores per subset.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b7973dc",
   "metadata": {},
   "source": [
    "## 1. Imports & Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf09044",
   "metadata": {},
   "source": [
    "This initial cell loads all the necessary libraries for the entire workflow. This includes:\n",
    "\n",
    "- pandas and numpy for data manipulation.\n",
    "- scikit-learn modules for data splitting (train_test_split), preprocessing (StandardScaler, OneHotEncoder, SimpleImputer, SplineTransformer), creating pipelines (Pipeline, ColumnTransformer), and building models (RidgeCV, RandomForestRegressor, SVR, StackingRegressor).\n",
    "XGBRegressor for the XGBoost model.\n",
    "- joblib for saving the final model pipelines.\n",
    "- A global RANDOM_STATE is set to 42 to ensure that any process with a random element (like data splitting or model training) is reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90472070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Imports & Settings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    KFold,\n",
    "    cross_val_score,\n",
    "    GridSearchCV,\n",
    ")\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Note: SimpleImputer lives in sklearn.impute, not preprocessing\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "# SplineTransformer was introduced in sklearn 1.0+.\n",
    "# If your version is older, you can install sklearn>=1.0 or substitute PolynomialFeatures.\n",
    "try:\n",
    "    from sklearn.preprocessing import SplineTransformer\n",
    "except ImportError:\n",
    "    SplineTransformer = None\n",
    "    print(\n",
    "        \"Warning: SplineTransformer not found. You can either upgrade scikit-learn to ≥1.0, \"\n",
    "        \"or replace SplineTransformer with PolynomialFeatures.\"\n",
    "    )\n",
    "\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import joblib  # for saving final pipelines\n",
    "\n",
    "# Global random seed\n",
    "RANDOM_STATE = 42\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f96b725",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Verification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee5cf71",
   "metadata": {},
   "source": [
    "Here, we load the primary dataset and performing essential checks.\n",
    "\n",
    "- It reads the data from ../data/processed/merged_climate_disease_final.csv into a pandas DataFrame.\n",
    "- It confirms the presence of the system_type column, which is critical for splitting the data later. The unique values are verified to be 'Natural' and 'Ag'.\n",
    "- Finally, it prints the shape of the full DataFrame and displays the first two rows to inspect the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63930151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique system_type values: ['Natural' 'Ag']\n",
      "Full DataFrame shape: (4932, 56)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>study_id</th>\n",
       "      <th>First_author</th>\n",
       "      <th>Study_Year</th>\n",
       "      <th>Person_extracting</th>\n",
       "      <th>Date_extracted</th>\n",
       "      <th>Paper_double_checked</th>\n",
       "      <th>study_reference</th>\n",
       "      <th>Host.species</th>\n",
       "      <th>Host_family</th>\n",
       "      <th>Host_order</th>\n",
       "      <th>...</th>\n",
       "      <th>start_date</th>\n",
       "      <th>monthly_temp</th>\n",
       "      <th>contemp_temp</th>\n",
       "      <th>contemp_precip</th>\n",
       "      <th>.geo</th>\n",
       "      <th>calculated_incidence</th>\n",
       "      <th>duration_mo</th>\n",
       "      <th>temp_anomaly</th>\n",
       "      <th>rain_anomaly</th>\n",
       "      <th>incidence_zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>715</td>\n",
       "      <td>Abbate</td>\n",
       "      <td>2014</td>\n",
       "      <td>Vianda</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>Devin</td>\n",
       "      <td>Abbate_2014</td>\n",
       "      <td>Silene vulgaris</td>\n",
       "      <td>Caryophyllaceae</td>\n",
       "      <td>Caryophyllales</td>\n",
       "      <td>...</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>181.284576</td>\n",
       "      <td>288.956314</td>\n",
       "      <td>0.005280</td>\n",
       "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[6.028999805...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>107.671738</td>\n",
       "      <td>-78.742580</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>715</td>\n",
       "      <td>Abbate</td>\n",
       "      <td>2014</td>\n",
       "      <td>Vianda</td>\n",
       "      <td>2021-06-30</td>\n",
       "      <td>Devin</td>\n",
       "      <td>Abbate_2014</td>\n",
       "      <td>Silene vulgaris</td>\n",
       "      <td>Caryophyllaceae</td>\n",
       "      <td>Caryophyllales</td>\n",
       "      <td>...</td>\n",
       "      <td>2008-08-01</td>\n",
       "      <td>133.753340</td>\n",
       "      <td>288.499283</td>\n",
       "      <td>0.005288</td>\n",
       "      <td>{\"type\":\"Polygon\",\"coordinates\":[[[6.117000102...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>154.745943</td>\n",
       "      <td>-96.430852</td>\n",
       "      <td>Low</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  study_id First_author  Study_Year Person_extracting Date_extracted  \\\n",
       "0      715       Abbate        2014            Vianda     2021-06-30   \n",
       "1      715       Abbate        2014            Vianda     2021-06-30   \n",
       "\n",
       "  Paper_double_checked study_reference     Host.species      Host_family  \\\n",
       "0                Devin     Abbate_2014  Silene vulgaris  Caryophyllaceae   \n",
       "1                Devin     Abbate_2014  Silene vulgaris  Caryophyllaceae   \n",
       "\n",
       "       Host_order  ...  start_date monthly_temp contemp_temp contemp_precip  \\\n",
       "0  Caryophyllales  ...  2008-08-01   181.284576   288.956314       0.005280   \n",
       "1  Caryophyllales  ...  2008-08-01   133.753340   288.499283       0.005288   \n",
       "\n",
       "                                                .geo calculated_incidence  \\\n",
       "0  {\"type\":\"Polygon\",\"coordinates\":[[[6.028999805...                  0.0   \n",
       "1  {\"type\":\"Polygon\",\"coordinates\":[[[6.117000102...                  0.0   \n",
       "\n",
       "  duration_mo temp_anomaly rain_anomaly incidence_zone  \n",
       "0           1   107.671738   -78.742580            Low  \n",
       "1           1   154.745943   -96.430852            Low  \n",
       "\n",
       "[2 rows x 56 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 2: Load & Initial Cleanup\n",
    "\n",
    "# 2.1 – Read the merged dataset (adjust path as needed)\n",
    "df = pd.read_csv(\"../data/processed/merged_climate_disease_final.csv\")\n",
    "\n",
    "# 2.2 – Verify “system_type” exists (should be “Wild” or “Agricultural”)\n",
    "if \"system_type\" not in df.columns:\n",
    "    raise ValueError(\n",
    "        \"DataFrame must contain 'system_type' column indicating 'Wild' or 'Agricultural'.\"\n",
    "    )\n",
    "\n",
    "print(\"Unique system_type values:\", df[\"system_type\"].unique())\n",
    "\n",
    "# 2.3 – Show shape and head\n",
    "print(\"Full DataFrame shape:\", df.shape)\n",
    "df.head(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc09baea",
   "metadata": {},
   "source": [
    "We load the complete DataFrame—including all climate, disease, and metadata columns—before dropping anything.\n",
    "\n",
    "We confirm that system_type (Wild vs. Agricultural) is present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bf4aec2",
   "metadata": {},
   "source": [
    "## 3. Defining the Target Variable and Feature Selection\n",
    "   \n",
    "This step isolates the target variable and removes columns that are not useful as predictors.\n",
    "\n",
    "- The target variable, y_cont, is set to the \"incidence\" column from the DataFrame.\n",
    "- A list named cols_to_drop is created to hold the names of columns to be removed. This includes the target variable itself, metadata (study_id, First_author), redundant date columns (Study_Year, start_date), and other non-predictive information.\n",
    "- These columns are dropped to create a new DataFrame, df_model, which contains only potential predictor features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01217f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after dropping non-predictors: (4932, 26)\n",
      "Remaining columns: ['Host.species', 'Host_family', 'Host_order', 'Host_type', 'Habitat', 'Parasite_or_pest', 'Antagonist_species', 'Antagonist_type_specific', 'Antagonist_type_general', 'Detection_method', 'Transmission_mode', 'Vector_type', 'Vector_species', 'Latitude', 'Longitude', 'location', 'Number_sampled_locations', 'annual_mean_temp', 'annual_precip', 'monthly_precip', 'monthly_temp', 'contemp_temp', 'contemp_precip', 'duration_mo', 'temp_anomaly', 'rain_anomaly']\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Define target (raw incidence) and drop non-predictors\n",
    "\n",
    "# 3.1 – Continuous target: raw incidence (e.g., fraction of infected plants)\n",
    "target_col = \"incidence\"\n",
    "y_cont = df[target_col]\n",
    "\n",
    "# 3.2 – Columns to drop (metadata / non-predictors)\n",
    "cols_to_drop = [\n",
    "    target_col,\n",
    "    \"incidence_zone\",\n",
    "    # Identifiers & metadata\n",
    "    \"study_id\",\n",
    "    \"First_author\",\n",
    "    \"Person_extracting\",\n",
    "    \"Date_extracted\",\n",
    "    \"Paper_double_checked\",\n",
    "    \"study_reference\",\n",
    "    \"system:index\",\n",
    "    \".geo\",\n",
    "    # Raw incidence/leakage (we only want \"incidence\" as target)\n",
    "    \"n_plants\",\n",
    "    \"n_infected\",\n",
    "    \"obs\",\n",
    "    \"Response metric\",\n",
    "    \"calculated_incidence\",\n",
    "    # Redundant time columns\n",
    "    \"Study_Year\",\n",
    "    \"Year\",\n",
    "    \"Months\",\n",
    "    \"Time_point\",\n",
    "    \"Start Date (yyyymm)\",\n",
    "    \"start_date\",\n",
    "    \"end_date\",\n",
    "    \"Span (months)\",\n",
    "    # Redundant spatial metadata\n",
    "    \"Coordinates_provided_in_study\",\n",
    "    \"Coarse_spatial_scale\",\n",
    "    \"Approx_spatial_accuracy_width_km\",\n",
    "    \"Location\",\n",
    "    # logistical metadata\n",
    "    \"Sample_type\",\n",
    "    \"Same_plants_tested_multiple\",\n",
    "    # We will split on system_type, so drop it here (we’ll still refer to df[\"system_type\"] above)\n",
    "    \"system_type\",\n",
    "]\n",
    "\n",
    "df_model = df.drop(columns=cols_to_drop, errors=\"ignore\")\n",
    "print(\"Shape after dropping non-predictors:\", df_model.shape)\n",
    "print(\"Remaining columns:\", df_model.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7fd647",
   "metadata": {},
   "source": [
    "We isolate y_cont (raw incidence) and remove everything non‐predictive or not needed.\n",
    "\n",
    "Note: We keep latitude, longitude, climate columns, host/antagonist, detection_method, etc.\n",
    "\n",
    "We also dropped system_type here, because we’ll use it only to split—after splitting, it’s not a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d238f0bd",
   "metadata": {},
   "source": [
    "## 4. Verifying Climate and Anomaly Features\n",
    "\n",
    "This cell ensures that all expected climate-related features are present and have no missing values.\n",
    "\n",
    "- A list, climate_cols, specifies all the required predictors, such as monthly_temp, annual_precip, temp_anomaly, and rain_anomaly.\n",
    "- The code checks if these columns exist in df_model and verifies that there are no null (NaN) values within them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "befa6768",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in climate columns:\n",
      "monthly_temp        0\n",
      "monthly_precip      0\n",
      "contemp_temp        0\n",
      "contemp_precip      0\n",
      "annual_mean_temp    0\n",
      "annual_precip       0\n",
      "temp_anomaly        0\n",
      "rain_anomaly        0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Verify that raw climate & anomaly features exist (or compute them)\n",
    "\n",
    "# List of expected climate columns:\n",
    "climate_cols = [\n",
    "    \"monthly_temp\",\n",
    "    \"monthly_precip\",\n",
    "    \"contemp_temp\",\n",
    "    \"contemp_precip\",\n",
    "    \"annual_mean_temp\",\n",
    "    \"annual_precip\",\n",
    "    \"temp_anomaly\",\n",
    "    \"rain_anomaly\",\n",
    "]\n",
    "\n",
    "# Ensure each exists; if not, compute from raw/historic data (adjust as needed)\n",
    "missing = [c for c in climate_cols if c not in df_model.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing expected climate columns: {missing}\")\n",
    "\n",
    "# Check for NaNs in climate features\n",
    "print(\"Missing values in climate columns:\")\n",
    "print(df_model[climate_cols].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4deb78aa",
   "metadata": {},
   "source": [
    "We verify that all the raw climate predictors (and their anomaly counterparts) are in place."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9fe843e",
   "metadata": {},
   "source": [
    "## 5. Feature Engineering: Creating Interaction and Quadratic Terms\n",
    "\n",
    "To capture more complex relationships, new features are engineered based on existing ones. This process is inspired by the methodology in the Kirk et al. study.\n",
    "\n",
    "- Quadratic Anomaly Terms: New columns temp_anomaly_sq and rain_anomaly_sq are created by squaring the anomaly values.\n",
    "- Interaction Terms: monthly_temp_x_temp_anomaly and monthly_precip_x_rain_anomaly are created by multiplying the monthly climate values by their corresponding anomalies.\n",
    "- Quadratic Raw Terms: monthly_temp_sq and monthly_precip_sq are also added by squaring the raw monthly temperature and precipitation values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58c06c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added interaction/quadratic columns status:\n",
      "  temp_anomaly_sq present? True\n",
      "  rain_anomaly_sq present? True\n",
      "  monthly_temp_x_temp_anomaly present? True\n",
      "  monthly_precip_x_rain_anomaly present? True\n",
      "  monthly_temp_sq present? True\n",
      "  monthly_precip_sq present? True\n",
      "\n",
      "Missing in new features:\n",
      "temp_anomaly_sq                  0\n",
      "rain_anomaly_sq                  0\n",
      "monthly_temp_x_temp_anomaly      0\n",
      "monthly_precip_x_rain_anomaly    0\n",
      "monthly_temp_sq                  0\n",
      "monthly_precip_sq                0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Add Kirk et al.–style interaction/quadratic features\n",
    "\n",
    "# 5.1 – Quadratic on anomaly (temp_diff^2, rain_diff^2)\n",
    "df_model[\"temp_anomaly_sq\"] = df_model[\"temp_anomaly\"] ** 2\n",
    "df_model[\"rain_anomaly_sq\"] = df_model[\"rain_anomaly\"] ** 2\n",
    "\n",
    "# 5.2 – Interaction: monthly_temp × temp_anomaly\n",
    "df_model[\"monthly_temp_x_temp_anomaly\"] = (\n",
    "    df_model[\"monthly_temp\"] * df_model[\"temp_anomaly\"]\n",
    ")\n",
    "\n",
    "# 5.3 – Interaction: monthly_precip × rain_anomaly\n",
    "df_model[\"monthly_precip_x_rain_anomaly\"] = (\n",
    "    df_model[\"monthly_precip\"] * df_model[\"rain_anomaly\"]\n",
    ")\n",
    "\n",
    "# 5.4 – (Optional) Quadratic on raw monthly_temp & monthly_precip\n",
    "df_model[\"monthly_temp_sq\"] = df_model[\"monthly_temp\"] ** 2\n",
    "df_model[\"monthly_precip_sq\"] = df_model[\"monthly_precip\"] ** 2\n",
    "\n",
    "# 5.5 – Confirm new columns\n",
    "new_cols = [\n",
    "    \"temp_anomaly_sq\",\n",
    "    \"rain_anomaly_sq\",\n",
    "    \"monthly_temp_x_temp_anomaly\",\n",
    "    \"monthly_precip_x_rain_anomaly\",\n",
    "    \"monthly_temp_sq\",\n",
    "    \"monthly_precip_sq\",\n",
    "]\n",
    "print(\"Added interaction/quadratic columns status:\")\n",
    "for c in new_cols:\n",
    "    print(f\"  {c} present? {c in df_model.columns}\")\n",
    "\n",
    "# 5.6 – Check missingness in these new columns\n",
    "print(\"\\nMissing in new features:\")\n",
    "print(df_model[new_cols].isnull().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f85aff2",
   "metadata": {},
   "source": [
    "We generate exactly the same sort of terms Kirk et al. found important:\n",
    "\n",
    "Quadratics on anomalies (temp_diff², rain_diff²)\n",
    "\n",
    "Cross‐terms between raw monthly climate and anomaly\n",
    "\n",
    "(Optionally) Quadratics on raw monthly values\n",
    "\n",
    "From this point onward, both the “Wild” and “Agri” subsets will include these extra columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ab5117",
   "metadata": {},
   "source": [
    "## 6. Assembling the Final Predictor Matrix (X_full)\n",
    "\n",
    "With feature engineering complete, this step finalizes the complete set of predictors.\n",
    "\n",
    "- The df_model DataFrame, which now contains the original predictors plus the newly created interaction and quadratic terms, is copied into a new DataFrame called X_full. This X_full represents the final predictor matrix before any data splitting occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d908a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X_full shape: (4932, 32)\n",
      "Final predictor columns: ['Host.species', 'Host_family', 'Host_order', 'Host_type', 'Habitat', 'Parasite_or_pest', 'Antagonist_species', 'Antagonist_type_specific', 'Antagonist_type_general', 'Detection_method', 'Transmission_mode', 'Vector_type', 'Vector_species', 'Latitude', 'Longitude', 'location', 'Number_sampled_locations', 'annual_mean_temp', 'annual_precip', 'monthly_precip', 'monthly_temp', 'contemp_temp', 'contemp_precip', 'duration_mo', 'temp_anomaly', 'rain_anomaly', 'temp_anomaly_sq', 'rain_anomaly_sq', 'monthly_temp_x_temp_anomaly', 'monthly_precip_x_rain_anomaly', 'monthly_temp_sq', 'monthly_precip_sq']\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Build the full predictor DataFrame (X_full) including all new features\n",
    "\n",
    "X_full = df_model.copy()\n",
    "print(\"Final X_full shape:\", X_full.shape)\n",
    "print(\"Final predictor columns:\", X_full.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3d937",
   "metadata": {},
   "source": [
    "We now have a master X_full that contains everything we want to feed into our pipelines—including all Kirk-style interactions.\n",
    "\n",
    "From here on, we’ll split X_full and y_cont into Wild/Agri and then into train/test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e55103",
   "metadata": {},
   "source": [
    "## 7. Splitting the Dataset by System Type\n",
    "\n",
    "A crucial step in the workflow is to divide the data into two independent subsets for separate modeling.\n",
    "\n",
    "- Boolean masks (mask_ag and mask_wd) are created based on the system_type column ('Ag' or 'Natural').\n",
    "- These masks are used to split X_full and the target y_cont into two distinct sets: X_ag_full/y_ag_full for agricultural data and X_wd_full/y_wd_full for wild data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1fb05b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shapes before trimming:\n",
      "  Agricultural full: (4155, 32) → (4155,)\n",
      "  Wild full:         (777, 32) → (777,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Split X_full, y_cont into Agricultural vs. Wild subsets\n",
    "\n",
    "mask_ag = df[\"system_type\"] == \"Ag\"\n",
    "mask_wd = df[\"system_type\"] == \"Natural\"\n",
    "\n",
    "X_ag_full = X_full[mask_ag].reset_index(drop=True)\n",
    "y_ag_full = y_cont[mask_ag].reset_index(drop=True)\n",
    "\n",
    "X_wd_full = X_full[mask_wd].reset_index(drop=True)\n",
    "y_wd_full = y_cont[mask_wd].reset_index(drop=True)\n",
    "\n",
    "print(\"Shapes before trimming:\")\n",
    "print(f\"  Agricultural full: {X_ag_full.shape} → {y_ag_full.shape}\")\n",
    "print(f\"  Wild full:         {X_wd_full.shape} → {y_wd_full.shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f4bf8a",
   "metadata": {},
   "source": [
    "We immediately split the data by system_type.\n",
    "\n",
    "Each subset (X_ag_full/y_ag_full and X_wd_full/y_wd_full) now contains exactly the rows where system_type was “Agricultural” or “Wild,” respectively.\n",
    "\n",
    "Notice that X_full still has the interaction/quadratic columns—those carry forward."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06babebb",
   "metadata": {},
   "source": [
    "## 8. Creating Training and Testing Sets\n",
    "\n",
    "- For both the agricultural and wild subsets, the data is further divided into training and testing sets.\n",
    "\n",
    "- The train_test_split function is used to create an 80% training and 20% testing split for each subset.\n",
    "This results in four pairs of DataFrames: X_ag_train, X_ag_test, y_ag_train, y_ag_test for the agricultural data, and X_wd_train, X_wd_test, y_wd_train, y_wd_test for the wild data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec23588e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Wild train/test shapes: (621, 32)  |  (156, 32)\n",
      "* Wild train/test groups: (621,)  |  (156,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Train/Test split (80/20) within each subset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 8.1 – Agricultural 80/20 split\n",
    "X_ag_train, X_ag_test, y_ag_train, y_ag_test = train_test_split(\n",
    "    X_ag_full, y_ag_full, test_size=0.20, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# 8.2 – Wild 80/20 split\n",
    "# X_wd_train, X_wd_test, y_wd_train, y_wd_test = train_test_split(\n",
    "#     X_wd_full, y_wd_full, test_size=0.20, random_state=RANDOM_STATE\n",
    "# )\n",
    "\n",
    "# print(f\"* Agri train/test shapes: {X_ag_train.shape}  |  {X_ag_test.shape}\")\n",
    "# print(f\"* Wild train/test shapes: {X_wd_train.shape}  |  {X_wd_test.shape}\\n\")\n",
    "\n",
    "# 8.2 – Wild 80/20 split (with grouping for later GroupKFold)\n",
    "\n",
    "# extract the study_id vector for Wild before splitting\n",
    "groups_wd_full = df[\"study_id\"].loc[X_wd_full.index]\n",
    "\n",
    "# Now split X, y, and groups together\n",
    "X_wd_train, X_wd_test, y_wd_train, y_wd_test, groups_wd_train, groups_wd_test = (\n",
    "    train_test_split(\n",
    "        X_wd_full, y_wd_full, groups_wd_full, test_size=0.20, random_state=RANDOM_STATE\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"* Wild train/test shapes: {X_wd_train.shape}  |  {X_wd_test.shape}\")\n",
    "print(f\"* Wild train/test groups: {groups_wd_train.shape}  |  {groups_wd_test.shape}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e22ef7",
   "metadata": {},
   "source": [
    "We treat each subset as a separate regression problem.\n",
    "\n",
    "From here onward we’ll build separate pipelines and do CV/test for Ag and for Wild independently."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0b0bf9",
   "metadata": {},
   "source": [
    "## 9. Identifying Numeric and Categorical Columns\n",
    "\n",
    "To prepare for preprocessing, the notebook automatically identifies which columns are numeric and which are categorical.\n",
    "\n",
    "- It inspects the data types (dtypes) of the columns in X_full to create numeric_cols and categorical_cols lists. This ensures that the newly engineered features (e.g., temp_anomaly_sq) are correctly identified as numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35164583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns (inferred):    ['Latitude', 'Longitude', 'Number_sampled_locations', 'annual_mean_temp', 'annual_precip', 'monthly_precip', 'monthly_temp', 'contemp_temp', 'contemp_precip', 'duration_mo', 'temp_anomaly', 'rain_anomaly', 'temp_anomaly_sq', 'rain_anomaly_sq', 'monthly_temp_x_temp_anomaly', 'monthly_precip_x_rain_anomaly', 'monthly_temp_sq', 'monthly_precip_sq']\n",
      "Categorical columns (inferred): ['Host.species', 'Host_family', 'Host_order', 'Host_type', 'Habitat', 'Parasite_or_pest', 'Antagonist_species', 'Antagonist_type_specific', 'Antagonist_type_general', 'Detection_method', 'Transmission_mode', 'Vector_type', 'Vector_species', 'location']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\baner\\AppData\\Local\\Temp\\ipykernel_7308\\3844641646.py:5: FutureWarning: is_categorical_dtype is deprecated and will be removed in a future version. Use isinstance(dtype, CategoricalDtype) instead\n",
      "  lambda dt: pd.api.types.is_object_dtype(dt) or pd.api.types.is_categorical_dtype(dt)\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Infer numeric_cols and categorical_cols from X_full\n",
    "\n",
    "numeric_dtype_mask = X_full.dtypes.apply(lambda dt: pd.api.types.is_numeric_dtype(dt))\n",
    "categorical_dtype_mask = X_full.dtypes.apply(\n",
    "    lambda dt: pd.api.types.is_object_dtype(dt) or pd.api.types.is_categorical_dtype(dt)\n",
    ")\n",
    "\n",
    "numeric_cols = X_full.columns[numeric_dtype_mask].tolist()\n",
    "categorical_cols = X_full.columns[categorical_dtype_mask].tolist()\n",
    "\n",
    "print(\"Numeric columns (inferred):   \", numeric_cols)\n",
    "print(\"Categorical columns (inferred):\", categorical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e568750b",
   "metadata": {},
   "source": [
    "We need to know which columns to impute/scale (numeric) and which to one‐hot (categorical).\n",
    "\n",
    "Because we added several new numerics (interactions, quadratics), they will appear in numeric_cols automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59093f06",
   "metadata": {},
   "source": [
    "## 10. Building Preprocessing Pipelines\n",
    "\n",
    "This section defines two distinct preprocessing pipelines to transform the data before it is fed into the models.\n",
    "\n",
    "- A numeric_pipeline is created to first impute missing numeric values using the median, and then scale the data using StandardScaler.\n",
    "- A categorical_pipeline is created to fill missing categorical values with the string \"Missing\" and then convert the categories into numerical format using OneHotEncoder.\n",
    "- A primary preprocessor combines these two pipelines using ColumnTransformer and is intended for tree-based models like Random Forest and XGBoost.\n",
    "- A second, more specialized preprocessor_spline is also built. It applies a SplineTransformer to key climate features, a standard scaler to other numeric features, and one-hot encoding to categorical features. This is designed for the RidgeCV model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59d5532b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipelines built successfully.\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Build ColumnTransformers for preprocessing\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# 10.1 – Numeric pipeline: median impute → standard scale\n",
    "numeric_pipeline = Pipeline(\n",
    "    [(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler())]\n",
    ")\n",
    "\n",
    "# 10.2 – Categorical pipeline: fill “Missing” → one-hot encode\n",
    "categorical_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10.3 – Combine into one preprocessor for tree-based pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", numeric_pipeline, numeric_cols),\n",
    "        (\"cat\", categorical_pipeline, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "# 10.4 – Build a \"Ridge-spline\" preprocessor:\n",
    "#       • Spline basis on core climate features\n",
    "#       • Impute & scale the rest of numeric_cols\n",
    "#       • Impute & one-hot encode categorical_cols\n",
    "\n",
    "climate_feats = [\"monthly_temp\", \"monthly_precip\", \"contemp_temp\", \"contemp_precip\"]\n",
    "other_nums = [c for c in numeric_cols if c not in climate_feats]\n",
    "\n",
    "# 10.4.1 – Spline subpipeline (only climate_feats)\n",
    "spline_climate_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"select_climate\",\n",
    "            ColumnTransformer(\n",
    "                [(\"clim\", \"passthrough\", climate_feats)], remainder=\"drop\"\n",
    "            ),\n",
    "        ),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"spline\", SplineTransformer(degree=3, n_knots=5, include_bias=False)),\n",
    "        (\"scale_spl\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10.4.2 – Other numeric subpipeline\n",
    "other_numeric_pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"select_other\",\n",
    "            ColumnTransformer([(\"oth\", \"passthrough\", other_nums)], remainder=\"drop\"),\n",
    "        ),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
    "        (\"scale_other\", StandardScaler()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10.4.3 – Categorical subpipeline\n",
    "categorical_pipeline_spl = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"select_cat\",\n",
    "            ColumnTransformer(\n",
    "                [(\"cat\", \"passthrough\", categorical_cols)], remainder=\"drop\"\n",
    "            ),\n",
    "        ),\n",
    "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
    "        (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\", sparse=False)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 10.4.4 – Combine into preprocessor_spline\n",
    "preprocessor_spline = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"spl_clim\", spline_climate_pipeline, climate_feats),\n",
    "        (\"spl_oth\", other_numeric_pipeline, other_nums),\n",
    "        (\"spl_cat\", categorical_pipeline_spl, categorical_cols),\n",
    "    ],\n",
    "    remainder=\"drop\",\n",
    ")\n",
    "\n",
    "print(\"Preprocessing pipelines built successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac232a2",
   "metadata": {},
   "source": [
    "We build two preprocessors:\n",
    "\n",
    "preprocessor for tree‐based models (RF, XGB, SVR), which simply imputes & one‐hot encodes.\n",
    "\n",
    "preprocessor_spline for Ridge‐spline, which applies spline‐basis to key climate features and scales the rest.\n",
    "\n",
    "Both pipelines ingest the full feature set, including all Kirk interaction columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64292cc",
   "metadata": {},
   "source": [
    "## 11 & 12. Defining Regression Models and Hyperparameter Grids\n",
    "\n",
    "Here, the full regression pipelines are assembled, and the search space for hyperparameter tuning is defined.\n",
    "\n",
    "- Model Pipelines: Four Pipeline objects are created: pipe_ridge_spline, pipe_rf_reg, pipe_xgb_reg, and pipe_svr. Each pipeline chains the appropriate preprocessor with a regression model.\n",
    "- Hyperparameter Grids: For each model, a parameter grid (e.g., param_grid_rf, param_grid_xgb) is defined. This grid specifies the different hyperparameter values that will be tested during model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e63c29f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression pipelines defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Define the four regression pipelines (Ridge-spline, RF, XGB, SVR)\n",
    "\n",
    "# 11.1 – Ridge + Splines\n",
    "pipe_ridge_spline = Pipeline(\n",
    "    [\n",
    "        (\"preproc\", preprocessor_spline),\n",
    "        (\"regressor\", RidgeCV(alphas=[0.01, 0.1, 1.0, 10.0], store_cv_values=True)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 11.2 – RandomForestRegressor\n",
    "pipe_rf_reg = Pipeline(\n",
    "    [\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\"regressor\", RandomForestRegressor(random_state=RANDOM_STATE)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 11.3 – XGBRegressor\n",
    "pipe_xgb_reg = Pipeline(\n",
    "    [\n",
    "        (\"preproc\", preprocessor),\n",
    "        (\n",
    "            \"regressor\",\n",
    "            XGBRegressor(\n",
    "                objective=\"reg:squarederror\", random_state=RANDOM_STATE, n_jobs=-1\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 11.4 – SVR\n",
    "pipe_svr = Pipeline([(\"preproc\", preprocessor), (\"regressor\", SVR())])\n",
    "\n",
    "print(\"Regression pipelines defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a54cc3",
   "metadata": {},
   "source": [
    "We now have one pipeline object per algorithm, using the preprocessors we defined.\n",
    "\n",
    "These will be passed into GridSearchCV (or StackingRegressor) separately for Wild and Agri."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce9886f",
   "metadata": {},
   "source": [
    "12. Hyperparameter Grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b6a3ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameter grids defined.\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Hyperparameter grids for each pipeline\n",
    "\n",
    "# 12.1 – Ridge-spline (via RidgeCV, so only alpha)\n",
    "param_grid_ridge = {\"regressor__alpha\": [0.01, 0.1, 1.0, 10.0]}\n",
    "\n",
    "# 12.2 – RandomForestRegressor\n",
    "param_grid_rf = {\n",
    "    \"regressor__n_estimators\": [50, 100, 200],\n",
    "    \"regressor__max_depth\": [None, 5, 15],\n",
    "    \"regressor__min_samples_split\": [2, 10],\n",
    "    \"regressor__min_samples_leaf\": [1, 5],\n",
    "}\n",
    "\n",
    "# 12.3 – XGBRegressor\n",
    "param_grid_xgb = {\n",
    "    \"regressor__n_estimators\": [30, 100, 200],\n",
    "    \"regressor__max_depth\": [3, 6, 10],\n",
    "    \"regressor__learning_rate\": [0.01, 0.1],\n",
    "    \"regressor__gamma\": [0, 0.1],\n",
    "}\n",
    "\n",
    "# 12.4 – SVR\n",
    "param_grid_svr = {\n",
    "    \"regressor__C\": [0.1, 1.0, 10.0],\n",
    "    \"regressor__epsilon\": [0.01, 0.1, 1.0],\n",
    "    \"regressor__kernel\": [\"rbf\", \"linear\"],\n",
    "    \"regressor__gamma\": [\"scale\", \"auto\"],\n",
    "}\n",
    "\n",
    "print(\"Hyperparameter grids defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9cc6a8",
   "metadata": {},
   "source": [
    "We now have four separate parameter grids, keyed exactly to each pipeline’s \"regressor__…\".\n",
    "\n",
    "We’ll use these in GridSearchCV inside each subset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72313fdb",
   "metadata": {},
   "source": [
    "## 13a. Performing Cross-Validation\n",
    "\n",
    "This cell evaluates and compares the performance of all defined models using 5-fold cross-validation on the training data.\n",
    "\n",
    "- A StackingRegressor is defined for both agricultural (stack_ag) and wild (stack_wd) subsets. This ensemble model combines the predictions of the XGBoost and Ridge-spline pipelines.\n",
    "- Using cross_val_score, the notebook runs 5-fold cross-validation for each of the four base models and the stacking regressor.\n",
    "- This process is performed independently for the agricultural training set and the wild training set.\n",
    "- The mean R² score is printed for each model on each subset, allowing for a direct comparison of their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f26533e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "→ 5‐fold CV on AGRICULTURAL:\n",
      "   Ridge‐spline (AG) R² folds = [0.56  0.457 0.55  0.565 0.519] | mean = 0.530\n",
      "   RandomForest (AG) R² folds = [0.575 0.512 0.585 0.593 0.531] | mean = 0.559\n",
      "   XGB (AG) R² folds        = [0.596 0.453 0.573 0.614 0.558] | mean = 0.559\n",
      "   SVR (AG) R² folds        = [0.568 0.443 0.56  0.541 0.551] | mean = 0.533\n",
      "   Stacking (AG) R² folds   = [0.596 0.479 0.588 0.62  0.572] | mean = 0.571\n",
      "\n",
      "→ 5‐fold CV on WILD:\n",
      "   Ridge‐spline (WD) R² folds = [0.497 0.608 0.627 0.364 0.601] | mean = 0.539\n",
      "   RandomForest (WD) R² folds = [0.476 0.52  0.564 0.313 0.452] | mean = 0.465\n",
      "   XGB (WD) R² folds        = [0.417 0.457 0.495 0.223 0.274] | mean = 0.373\n",
      "   SVR (WD) R² folds        = [0.354 0.532 0.611 0.351 0.547] | mean = 0.479\n",
      "   Stacking (WD) R² folds   = [0.491 0.588 0.61  0.368 0.543] | mean = 0.520\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: CV on each subset, comparing the four models + Stacking\n",
    "# ───────────────────────────────────────────────────────────────────\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# 13.1 – Initialize 5‐fold CV for continuous targets\n",
    "cv5 = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# 13.2 – Define the “Stacking” meta‐pipeline for Agri and Wild (fresh copies each)\n",
    "\n",
    "# 13.2.1 – For AGRICULTURAL\n",
    "pipe_xgb_ag = pipe_xgb_reg  # uses same preprocessor, same hyperparams globally tuned\n",
    "pipe_ridge_ag = pipe_ridge_spline\n",
    "\n",
    "stack_ag = StackingRegressor(\n",
    "    estimators=[(\"xgb\", pipe_xgb_ag), (\"r_spl\", pipe_ridge_ag)],\n",
    "    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]),\n",
    "    n_jobs=-1,\n",
    "    passthrough=False,\n",
    ")\n",
    "\n",
    "# 13.2.2 – For WILD\n",
    "pipe_xgb_wd = pipe_xgb_reg\n",
    "pipe_ridge_wd = pipe_ridge_spline\n",
    "\n",
    "stack_wd = StackingRegressor(\n",
    "    estimators=[(\"xgb\", pipe_xgb_wd), (\"r_spl\", pipe_ridge_wd)],\n",
    "    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]),\n",
    "    n_jobs=-1,\n",
    "    passthrough=False,\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 13.3 – Run CV for each model on AGRICULTURAL subset\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"→ 5‐fold CV on AGRICULTURAL:\")\n",
    "\n",
    "# 13.3.1 – Ridge‐spline (AG)\n",
    "scores_ridge_ag = cross_val_score(\n",
    "    pipe_ridge_ag, X_ag_train, y_ag_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   Ridge‐spline (AG) R² folds = {np.round(scores_ridge_ag, 3)} | mean = {scores_ridge_ag.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.3.2 – RF (AG)\n",
    "scores_rf_ag = cross_val_score(\n",
    "    pipe_rf_reg, X_ag_train, y_ag_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   RandomForest (AG) R² folds = {np.round(scores_rf_ag, 3)} | mean = {scores_rf_ag.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.3.3 – XGB (AG)\n",
    "scores_xgb_ag = cross_val_score(\n",
    "    pipe_xgb_reg, X_ag_train, y_ag_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   XGB (AG) R² folds        = {np.round(scores_xgb_ag, 3)} | mean = {scores_xgb_ag.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.3.4 – SVR (AG)\n",
    "scores_svr_ag = cross_val_score(\n",
    "    pipe_svr, X_ag_train, y_ag_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   SVR (AG) R² folds        = {np.round(scores_svr_ag, 3)} | mean = {scores_svr_ag.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.3.5 – Stacking (AG)\n",
    "scores_stack_ag = cross_val_score(\n",
    "    stack_ag, X_ag_train, y_ag_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   Stacking (AG) R² folds   = {np.round(scores_stack_ag, 3)} | mean = {scores_stack_ag.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "# 13.4 – Run CV for each model on WILD subset\n",
    "# ─────────────────────────────────────────────────────────────────────────────\n",
    "print(\"\\n→ 5‐fold CV on WILD:\")\n",
    "\n",
    "# 13.4.1 – Ridge‐spline (WD)\n",
    "scores_ridge_wd = cross_val_score(\n",
    "    pipe_ridge_wd, X_wd_train, y_wd_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   Ridge‐spline (WD) R² folds = {np.round(scores_ridge_wd, 3)} | mean = {scores_ridge_wd.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.4.2 – RF (WD)\n",
    "scores_rf_wd = cross_val_score(\n",
    "    pipe_rf_reg, X_wd_train, y_wd_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   RandomForest (WD) R² folds = {np.round(scores_rf_wd, 3)} | mean = {scores_rf_wd.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.4.3 – XGB (WD)\n",
    "scores_xgb_wd = cross_val_score(\n",
    "    pipe_xgb_reg, X_wd_train, y_wd_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   XGB (WD) R² folds        = {np.round(scores_xgb_wd, 3)} | mean = {scores_xgb_wd.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.4.4 – SVR (WD)\n",
    "scores_svr_wd = cross_val_score(\n",
    "    pipe_svr, X_wd_train, y_wd_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   SVR (WD) R² folds        = {np.round(scores_svr_wd, 3)} | mean = {scores_svr_wd.mean():.3f}\"\n",
    ")\n",
    "\n",
    "# 13.4.5 – Stacking (WD)\n",
    "scores_stack_wd = cross_val_score(\n",
    "    stack_wd, X_wd_train, y_wd_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\n",
    "    f\"   Stacking (WD) R² folds   = {np.round(scores_stack_wd, 3)} | mean = {scores_stack_wd.mean():.3f}\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95164e72",
   "metadata": {},
   "source": [
    "We do five‐fold cross‐validation on the four base pipelines (Ridge-spline, RF, XGB, SVR) plus the stacking ensemble—once on each subset’s training data.\n",
    "\n",
    "This immediately shows whether splitting by system type and including interactions pushes the CV R² higher than the global ~0.54."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa07c411",
   "metadata": {},
   "source": [
    "Test-Set Evaluation for Wild & Agricultural"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5bf90d",
   "metadata": {},
   "source": [
    "**1.1 Agricultural Subset (n ≈ 3,500 after trimming)**\n",
    "\n",
    "Stacking (0.571) is best, narrowly beating XGB and RF alone (0.559).\n",
    "\n",
    "The individual learners all cluster between ~0.51–0.54. Stacking adds roughly +0.015–0.020 in mean CV R².\n",
    "\n",
    "There is relatively low fold‐to‐fold variance (std ≈ 0.020–0.030) in the Agricultural folds, indicating the model is fairly stable on that ~3,500‐sample dataset.\n",
    "\n",
    "Takeaways for Agricultural\n",
    "Stacking is clearly the top performer here (0.554 vs. 0.539 for XGB alone). You can safely choose Stacking(XGB + Ridge) as your “production” model for crops.\n",
    "\n",
    "A mean CV R² ≈ 0.55 aligns with expectations: Kirk et al. reported a conditional R² ≈ 0.60 (when combining temperature and precipitation mixed models), so ~0.55 from an ML pipeline without random effects is quite respectable.\n",
    "\n",
    "If we want more gain, we might try:\n",
    "• Expanding hyperparameter grids for XGB/RF (you ran a moderately sized grid; you could try deeper trees, learning rate adjustments, or extra regularization).\n",
    "• Tweaking the Ridge‐spline model’s knot placement or degree—sometimes alternative spline transforms (e.g. natural splines) yield slight improvements.\n",
    "• Trying a LightGBM pipeline or CatBoost pipeline, which can sometimes edge out XGB on tabular data.\n",
    "\n",
    "**1.2 Wild Subset (n ≈ 600 after trimming)**\n",
    "\n",
    "Ridge-Spline obtains mean CV R² ≈ 0.539.\n",
    "\n",
    "- Stacking is second‐best at 0.520 (slightly higher than Ridge-Spline; but note the much bigger fold‐to‐fold swing: one fold is as low as 0.303).\n",
    "\n",
    "- SVR is next at 0.479, also with substantial variance across folds.\n",
    "\n",
    "RF and XGB individually collapse to ~0.373 and ~0.465, respectively—folds vary hugely from ~0.58 down to ~0.16.\n",
    "\n",
    "**Why Wild performance is lower and unstable?**\n",
    "\n",
    "- Small sample size (n≈600). With only ~480 training rows (80% of ~600) in each CV iteration, tree‐based learners (RF/XGB) are prone to high variance. One fold may happen to include an outlier or a cluster of rare host‐antagonist combinations that degrade performance dramatically.\n",
    "\n",
    "- Heterogeneity in Wild systems. The wild‐systems dataset likely contains many different host taxa, detection methods, and geographic regions—all mixed together. Despite trimming and adding interactions, there is still unmodeled hierarchical structure (study ID, host order) that a pure‐ML pipeline cannot capture, causing inconsistent CV splits.\n",
    "\n",
    "- Model hyperparameters are global, not wild‐specific. We re‐used the same XGB and Ridge hyperparameters found on the full dataset for both subsets. But the Wild subset has different variance and signal‐to‐noise characteristics, so those hyperparameters may not be optimal.\n",
    "\n",
    "- Extreme fold‐to‐fold swings. Notice the Ridge‐Spline foldds: [0.505, 0.565, 0.629, 0.579, 0.303]. That last fold at 0.303 indicates that one of the held‐out Wild folds contained a cluster of observations that the spline basis (or the features themselves) simply could not predict well—likely very low or high incidence under unusual climate.\n",
    "\n",
    "- Because of these factors, the mean Wild CV R² ≈ 0.508 is lower than the ~0.60–0.65 we expected, and it will likely remain volatile unless we take further steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bc22af",
   "metadata": {},
   "source": [
    "## 13c. Re-applying earlier “favorite” AG stacking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019bb77b",
   "metadata": {},
   "source": [
    "My original stacking hyper-parameters were better than the tuned ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b18ede1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking (AG) default CV R²: 0.5710910850528862\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 13b – Confirm Default Stacking CV on Agri ───\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Rebuild stack_ag with default XGB + default RidgeCV\n",
    "stack_ag = StackingRegressor(\n",
    "    estimators=[\n",
    "        (\"xgb\", pipe_xgb_ag),  # XGBRegressor with default params\n",
    "        (\"r_spl\", pipe_ridge_ag),  # RidgeCV(alphas=[0.1,1.0,10.0])\n",
    "    ],\n",
    "    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]),\n",
    "    n_jobs=-1,\n",
    "    passthrough=False,\n",
    ")\n",
    "\n",
    "# Evaluate with plain 5-fold CV\n",
    "scores_stack_ag = cross_val_score(\n",
    "    stack_ag, X_ag_train, y_ag_train, cv=cv5, scoring=\"r2\", n_jobs=-1\n",
    ")\n",
    "print(\"Stacking (AG) default CV R²:\", scores_stack_ag.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e40bbf",
   "metadata": {},
   "source": [
    "## 13b. Hyperparameter Tuning for Production Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112f8dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    }
   ],
   "source": [
    "# ─── Cell 13b – Hyperparameter Tuning for Production Models (plain KFold for both) ───\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 13b.1 – Tune Stacking on AGRICULTURAL\n",
    "stack_ag = StackingRegressor(\n",
    "    estimators=[(\"xgb\", pipe_xgb_ag), (\"r_spl\", pipe_ridge_ag)],\n",
    "    final_estimator=RidgeCV(alphas=[0.1, 1.0, 10.0]),\n",
    "    n_jobs=-1,\n",
    "    passthrough=False,\n",
    ")\n",
    "param_grid_stack_ag = {\n",
    "    \"xgb__regressor__learning_rate\": [0.01, 0.1],\n",
    "    \"xgb__regressor__max_depth\": [3, 5],\n",
    "    \"r_spl__regressor__alphas\": [[0.001, 0.01, 0.1], [0.01, 0.1, 1.0]],\n",
    "}\n",
    "\n",
    "cv5 = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "grid_stack_ag = GridSearchCV(\n",
    "    stack_ag, param_grid_stack_ag, cv=cv5, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_stack_ag.fit(X_ag_train, y_ag_train)\n",
    "print(\"AG Stacking best params:\", grid_stack_ag.best_params_)\n",
    "print(\"AG Stacking 5-fold R²:\", grid_stack_ag.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f89e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Cell N+1: Compute mean R² for each model ─────────────────────────────────\n",
    "\n",
    "# Agricultural\n",
    "r2_ag_ridge = scores_ridge_ag.mean()\n",
    "r2_ag_rf    = scores_rf_ag.mean()\n",
    "r2_ag_xgb   = scores_xgb_ag.mean()\n",
    "r2_ag_svr   = scores_svr_ag.mean()\n",
    "r2_ag_stack = scores_stack_ag.mean()\n",
    "\n",
    "# Wild\n",
    "r2_wd_ridge = scores_ridge_wd.mean()\n",
    "r2_wd_rf    = scores_rf_wd.mean()\n",
    "r2_wd_xgb   = scores_xgb_wd.mean()\n",
    "r2_wd_svr   = scores_svr_wd.mean()\n",
    "r2_wd_stack = scores_stack_wd.mean()\n",
    "\n",
    "# Confirm\n",
    "print(f\"Ag R²: Ridge={r2_ag_ridge:.3f}, RF={r2_ag_rf:.3f}, XGB={r2_ag_xgb:.3f}, SVR={r2_ag_svr:.3f}, Stack={r2_ag_stack:.3f}\")\n",
    "print(f\"Wd R²: Ridge={r2_wd_ridge:.3f}, RF={r2_wd_rf:.3f}, XGB={r2_wd_xgb:.3f}, SVR={r2_wd_svr:.3f}, Stack={r2_wd_stack:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2735e282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\baner\\OneDrive\\Documenti\\data_analytics_AI\\capstone_CI_main\\climacrop_health\\climacrop_health\\.venv\\Lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:975: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WD Ridge best params: {'preproc__spl_clim__spline__degree': 3, 'preproc__spl_clim__spline__n_knots': 5, 'regressor__alphas': [0.1, 1.0, 10.0]}\n",
      "WD Ridge 5-fold R²: 0.5393715029993846\n"
     ]
    }
   ],
   "source": [
    "# 13b.2 – Tune Ridge‐Spline on WILD using the same plain KFold\n",
    "param_grid_ridge_wd = {\n",
    "    \"preproc__spl_clim__spline__n_knots\": [5, 7, 9],\n",
    "    \"preproc__spl_clim__spline__degree\": [3, 4],\n",
    "    \"regressor__alphas\": [[0.001, 0.01, 0.1], [0.01, 0.1, 1.0], [0.1, 1.0, 10.0]],\n",
    "}\n",
    "\n",
    "kf_wd = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "grid_ridge_wd = GridSearchCV(\n",
    "    pipe_ridge_wd, param_grid_ridge_wd, cv=kf_wd, scoring=\"r2\", n_jobs=-1, verbose=1\n",
    ")\n",
    "grid_ridge_wd.fit(X_wd_train, y_wd_train)\n",
    "print(\"WD Ridge best params:\", grid_ridge_wd.best_params_)\n",
    "print(\"WD Ridge 5-fold R²:\", grid_ridge_wd.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b85867e",
   "metadata": {},
   "source": [
    "## 14. Final Model Evaluation on the Test Set\n",
    "\n",
    "After identifying the best model from cross-validation (assumed to be the stacking model), this step assesses its performance on unseen test data.\n",
    "\n",
    "- The champion models (stack_ag and ridge_wd) are fitted on their respective full training datasets (X_ag_train, X_wd_train).\n",
    "- Predictions are made on the corresponding held-out test sets (X_ag_test, X_wd_test).\n",
    "- The final Mean Squared Error (MSE) and R² scores are calculated and printed, providing a definitive measure of the models' predictive power on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1086c4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stack_ag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Fit on full Agri train split\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mstack_ag\u001b[49m.fit(X_ag_train, y_ag_train)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Predict & score on Agri test split\u001b[39;00m\n\u001b[32m      8\u001b[39m y_ag_pred = stack_ag.predict(X_ag_test)\n",
      "\u001b[31mNameError\u001b[39m: name 'stack_ag' is not defined"
     ]
    }
   ],
   "source": [
    "# ─── Cell 14 – Final Test-Set Evaluation of Tuned Models ───\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Fit on full Agri train split\n",
    "stack_ag.fit(X_ag_train, y_ag_train)\n",
    "\n",
    "# Predict & score on Agri test split\n",
    "y_ag_pred = stack_ag.predict(X_ag_test)\n",
    "mse_ag = mean_squared_error(y_ag_test, y_ag_pred)\n",
    "r2_ag = r2_score(y_ag_test, y_ag_pred)\n",
    "\n",
    "print(f\"[AG TEST] Default Stacking    MSE = {mse_ag:.5f}   |   R² = {r2_ag:.3f}\")\n",
    "\n",
    "\n",
    "# 14.2 – WILD: tuned Ridge-Spline\n",
    "best_ridge_wd = grid_ridge_wd.best_estimator_\n",
    "y_wd_pred = best_ridge_wd.predict(X_wd_test)\n",
    "mse_wd = mean_squared_error(y_wd_test, y_wd_pred)\n",
    "r2_wd = r2_score(y_wd_test, y_wd_pred)\n",
    "print(f\"[WD TEST] Tuned Ridge-Spline MSE = {mse_wd:.5f}   |   R² = {r2_wd:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745a7b5",
   "metadata": {},
   "source": [
    "We fit the best CV‐found pipeline (Stacking) on the entire training split of each subset, then evaluate on that subset’s held‐out test set.\n",
    "\n",
    "\n",
    "**Wild Test R² ≈ MSE = 0.03014   |   R² = 0.516**\n",
    "\n",
    "**Agri Test R² ≈ MSE = 0.02879   |   R² = 0.527**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609affb6",
   "metadata": {},
   "source": [
    "## 15. Feature Importance Analysis\n",
    "\n",
    "The final step is to investigate which features are most important for the best-performing models in each subset.\n",
    "\n",
    "- permutation_importance is used to calculate feature importance for the fitted stacking models on the test data for both the agricultural and wild subsets.\n",
    "- The results are stored in pandas DataFrames, and the top 10 most influential features for each model are sorted and printed. This reveals the key drivers of disease incidence specific to each ecosystem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ec73cc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stack_ag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minspection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m permutation_importance\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 15.1 – Agricultural permutation importance\u001b[39;00m\n\u001b[32m      6\u001b[39m perm_ag = permutation_importance(\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m     \u001b[43mstack_ag\u001b[49m, X_ag_test, y_ag_test, n_repeats=\u001b[32m10\u001b[39m, random_state=RANDOM_STATE, n_jobs=-\u001b[32m1\u001b[39m\n\u001b[32m      8\u001b[39m )\n\u001b[32m      9\u001b[39m imp_df_ag = (\n\u001b[32m     10\u001b[39m     pd.DataFrame({\u001b[33m\"\u001b[39m\u001b[33mfeature\u001b[39m\u001b[33m\"\u001b[39m: X_ag_test.columns, \u001b[33m\"\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m\"\u001b[39m: perm_ag.importances_mean})\n\u001b[32m     11\u001b[39m     .sort_values(\u001b[33m\"\u001b[39m\u001b[33mimportance\u001b[39m\u001b[33m\"\u001b[39m, ascending=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     12\u001b[39m     .head(\u001b[32m10\u001b[39m)\n\u001b[32m     13\u001b[39m )\n\u001b[32m     15\u001b[39m plt.figure()\n",
      "\u001b[31mNameError\u001b[39m: name 'stack_ag' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# 15.1 – Agricultural permutation importance\n",
    "perm_ag = permutation_importance(\n",
    "    stack_ag, X_ag_test, y_ag_test, n_repeats=10, random_state=RANDOM_STATE, n_jobs=-1\n",
    ")\n",
    "imp_df_ag = (\n",
    "    pd.DataFrame({\"feature\": X_ag_test.columns, \"importance\": perm_ag.importances_mean})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(imp_df_ag[\"feature\"][::-1], imp_df_ag[\"importance\"][::-1])\n",
    "plt.xlabel(\"Mean Permutation Importance\")\n",
    "plt.title(\"Top 10 Agricultural Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 15.2 – Wild permutation importance\n",
    "perm_wd = permutation_importance(\n",
    "    best_ridge_wd,\n",
    "    X_wd_test,\n",
    "    y_wd_test,\n",
    "    n_repeats=10,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "imp_df_wd = (\n",
    "    pd.DataFrame({\"feature\": X_wd_test.columns, \"importance\": perm_wd.importances_mean})\n",
    "    .sort_values(\"importance\", ascending=False)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.barh(imp_df_wd[\"feature\"][::-1], imp_df_wd[\"importance\"][::-1])\n",
    "plt.xlabel(\"Mean Permutation Importance\")\n",
    "plt.title(\"Top 10 Wild Features\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ab245",
   "metadata": {},
   "source": [
    "We will also export this figure for dashboarding purposes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8071c820",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'imp_df_ag' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ── Export feature‐importance tables ───────────────────────────────────────\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mimp_df_ag\u001b[49m.to_html(\u001b[33m\"\u001b[39m\u001b[33m../images/feature_importance_ag.html\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      3\u001b[39m imp_df_wd.to_html(\u001b[33m\"\u001b[39m\u001b[33m../images/feature_importance_wd.html\u001b[39m\u001b[33m\"\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mExported feature_importance_ag.html and feature_importance_wd.html\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'imp_df_ag' is not defined"
     ]
    }
   ],
   "source": [
    "# ── Export feature‐importance tables ───────────────────────────────────────\n",
    "imp_df_ag.to_html(\"../images/feature_importance_ag.html\", index=False)\n",
    "imp_df_wd.to_html(\"../images/feature_importance_wd.html\", index=False)\n",
    "print(\"Exported feature_importance_ag.html and feature_importance_wd.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bdbab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ── Gather your four R² scores ────────────────────────────────────────────\n",
    "r2_dict = {\n",
    "    \"Ag: Stacking\":     r2_ag_stack,\n",
    "    \"Ag: RandomForest\": r2_ag_rf,\n",
    "    \"Wd: Ridge\":        r2_wd_ridge,\n",
    "    \"Wd: Stacking\":     r2_wd_stack,\n",
    "}\n",
    "r2_df = (\n",
    "    pd.DataFrame.from_dict(r2_dict, orient=\"index\", columns=[\"R2\"])\n",
    "      .reset_index().rename(columns={\"index\":\"Model\"})\n",
    ")\n",
    "r2_df.to_html(\"../images/model_r2_scores.html\", index=False)\n",
    "print(\"Exported model_r2_scores.html\")\n",
    "\n",
    "# ── Plot R² comparison ────────────────────────────────────────────────────\n",
    "\n",
    "plt.figure(figsize=(8,4))\n",
    "plt.barh(r2_df[\"Model\"], r2_df[\"R2\"], color=\"#2E7D32\")\n",
    "plt.xlabel(\"Test R²\")\n",
    "plt.title(\"Model Comparison: Test R²\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"images/model_r2_comparison.png\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# ── Announce best model ───────────────────────────────────────────────────\n",
    "best = r2_df.loc[r2_df[\"R2\"].idxmax()]\n",
    "print(f\"Best model: {best['Model']} (R² = {best['R2']:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9248a0",
   "metadata": {},
   "source": [
    "**Feature Importance Differs by System:**\n",
    "\n",
    "- The permutation importance analysis reveals that the two models rely on different types of predictors.\n",
    "- The Agricultural model gives higher importance to biological and metadata features, such as Host_family, Antagonist_species, and Host.species.\n",
    "- The Wild model relies more heavily on raw and anomaly-based climate features, like contemp_precip, monthly_temp_sq, and rain_anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d72cc98",
   "metadata": {},
   "source": [
    "## **16. Insights & Recommendations**\n",
    "\n",
    "**Causes of Performance Reduction on Hold-Out Test Sets**\n",
    "\n",
    "- Single Hold-Out Variance: The 80/20 train/test split may by chance allocate particularly challenging site-year observations to the test sets, leading to lower R² compared to average cross-validation.\n",
    "\n",
    "- Over-Tuning on CV: Hyperparameter tuning on limited folds can overfit to CV splits, hurting generalization for one static test partition.\n",
    "\n",
    "- Data Heterogeneity: Both agricultural and wild subsets mix diverse hosts, regions, and years; edge-case combinations in the test sets can lie outside the training distribution.\n",
    "\n",
    "**Recommendations for Future Performance Enhancement**\n",
    "\n",
    "- Stabilize Performance Estimates\n",
    "\n",
    "- Use RepeatedKFold or RepeatedStratifiedKFold to average results over multiple random splits.\n",
    "\n",
    "- Conduct multiple train/test rounds and report mean ± standard deviation for test R².\n",
    "\n",
    "**Augment Feature Space**\n",
    "\n",
    "- Introduce additional interaction features (e.g., climate × host order, temperature × precipitation anomalies).\n",
    "\n",
    "- Include geospatial indicators (e.g., latitude, longitude, climate zone) or clustering features to capture regional effects.\n",
    "\n",
    "**Leverage Hierarchical Modeling**\n",
    "\n",
    "- Incorporate random effects for study ID or host taxa via a mixed-effects model (statsmodels or glmmTMB), then stack its residuals into the ML pipeline.\n",
    "\n",
    "- Alternatively, encode study_id or host_order through target or embedding encodings in tree-based learners.\n",
    "\n",
    "**Explore Alternative Boosters**\n",
    "\n",
    "- Test LightGBM or CatBoost instead of XGBoost in the stacking ensemble; these can sometimes edge out XGB on tabular data.\n",
    "\n",
    "**Regularization & Complexity Control**\n",
    "\n",
    "- For Ridge-spline on wild, experiment with natural splines or fewer knots (e.g., 3–5) to reduce overfitting.\n",
    "\n",
    "- Apply early stopping in gradient boosting and tighter min_samples_leaf for RandomForest to improve generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9363b",
   "metadata": {},
   "source": [
    ">Logically, I have also tried to explain the justification of performance for the ML models used in this study relative \n",
    ">to the source paper authored by Kirk et al. in notebook **05_comparative_analysis_Kirk_et_al.ipynb**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90473484",
   "metadata": {},
   "source": [
    "Our ML pipeline provides a robust, data‐driven benchmark (R²≈0.57 Agri; R²≈0.52 Wild), but falls short of the mixed‐model conditional R² (~0.63 and ~0.93) due to its reliance on fixed‐effects spline and decision‐tree learners alone. Incorporating hierarchical modeling (random effects) or engineered study‐level features, along with expanded sample sizes for Wild systems, is the most promising path to closing this performance gap."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
